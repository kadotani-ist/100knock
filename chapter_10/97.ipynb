{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "97.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjqH6Bic-FZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "27f9291c-e98b-4b78-bfa2-aa6754989b2d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at ./drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHpuZhaR-y7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "f486083f-292c-4ee9-aa83-3404fc65c171"
      },
      "source": [
        "!pip install fairseq tensorboardX"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 2.8MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.38.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2035370 sha256=f785293220e704f1342ac3edc197cfa719a4f0ddb57535040b7830812709ba2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq, tensorboardX\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.0 sacrebleu-1.4.9 tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4B1h57A-0MW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "600efa5a-1512-466b-89c9-fb600ec8655c"
      },
      "source": [
        "! fairseq-train 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin' \\\n",
        "  --save-dir 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-2' \\\n",
        "  --fp16 \\\n",
        "  --arch transformer --share-decoder-input-output-embed \\\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "  --optimizer adam --clip-norm 1.0 \\\n",
        "  --lr 1e-2 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
        "  --weight-decay 1e-4 \\\n",
        "  --dropout 0.2 \\\n",
        "  --max-epoch 5 \\\n",
        "  --max-tokens 8000 \\\n",
        "  --tensorboard-logdir 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_runs_-2'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=1.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.01], lr_scheduler='inverse_sqrt', max_epoch=5, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=8000, max_tokens_valid=8000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-2', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_runs_-2', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=-1, warmup_updates=2000, weight_decay=0.0001)\n",
            "| [ja] dictionary: 70272 types\n",
            "| [en] dictionary: 59088 types\n",
            "| loaded 1166 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/valid.ja-en.ja\n",
            "| loaded 1166 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/valid.ja-en.en\n",
            "| drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin valid ja-en 1166 examples\n",
            "TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): Embedding(70272, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(59088, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "| model transformer, criterion LabelSmoothedCrossEntropyCriterion\n",
            "| num. model params: 110370816 (num. trained: 110370816)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = 8000 and max sentences per GPU = None\n",
            "| no existing checkpoint found drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-2/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 440288 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/train.ja-en.ja\n",
            "| loaded 440288 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/train.ja-en.en\n",
            "| drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin train ja-en 440288 examples\n",
            "| WARNING: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster\n",
            "| epoch 001:   0% 0/1710 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 64.0\n",
            "| epoch 001:   0% 1/1710 [00:01<32:25,  1.14s/it]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "| epoch 001:   0% 2/1710 [00:01<27:08,  1.05it/s, loss=16.482, nll_loss=16.472, ppl=90908.8, wps=926, ups=0, wpb=6646.000, bsz=80.000, num_updates=1, lr=5e-06, gnorm=7.514, clip=1.000, oom=0.000, loss_scale=64.000, wall=7, train_wall=1]| WARNING: overflow detected, setting loss scale to: 32.0\n",
            "| epoch 001:   9% 161/1710 [01:29<14:03,  1.84it/s, loss=11.610, nll_loss=10.927, ppl=1946.83, wps=11476, ups=2, wpb=6889.553, bsz=235.019, num_updates=159, lr=0.000795, gnorm=1.668, clip=0.692, oom=0.000, loss_scale=32.000, wall=95, train_wall=89]| WARNING: overflow detected, setting loss scale to: 16.0\n",
            "| epoch 001:  11% 192/1710 [01:46<13:54,  1.82it/s, loss=11.501, nll_loss=10.799, ppl=1781.29, wps=11528, ups=2, wpb=6812.339, bsz=235.937, num_updates=189, lr=0.000945, gnorm=1.597, clip=0.672, oom=0.000, loss_scale=16.000, wall=112, train_wall=105]| WARNING: overflow detected, setting loss scale to: 8.0\n",
            "| epoch 001:  18% 314/1710 [02:53<12:42,  1.83it/s, loss=11.256, nll_loss=10.511, ppl=1458.94, wps=11749, ups=2, wpb=6798.781, bsz=240.284, num_updates=310, lr=0.00155, gnorm=1.332, clip=0.523, oom=0.000, loss_scale=8.000, wall=179, train_wall=172] | WARNING: overflow detected, setting loss scale to: 4.0\n",
            "| epoch 001 | loss 10.769 | nll_loss 9.956 | ppl 993.58 | wps 12446 | ups 2 | wpb 6968.859 | bsz 254.470 | num_updates 1705 | lr 0.008525 | gnorm 0.656 | clip 0.150 | oom 0.000 | loss_scale 4.000 | wall 955 | train_wall 940\n",
            "| epoch 001 | valid on 'valid' subset | loss 10.895 | nll_loss 10.015 | ppl 1034.59 | num_updates 1705\n",
            "| saved checkpoint drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-2/checkpoint1.pt (epoch 1 @ 1705 updates) (writing took 57.68327760696411 seconds)\n",
            "| epoch 002 | loss 10.713 | nll_loss 9.902 | ppl 956.5 | wps 12577 | ups 2 | wpb 6970.249 | bsz 257.478 | num_updates 3415 | lr 0.00765279 | gnorm 0.351 | clip 0.021 | oom 0.000 | loss_scale 4.000 | wall 1962 | train_wall 1878\n",
            "| epoch 002 | valid on 'valid' subset | loss 10.721 | nll_loss 9.907 | ppl 960.36 | num_updates 3415 | best_loss 10.7207\n",
            "| saved checkpoint drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-2/checkpoint2.pt (epoch 2 @ 3415 updates) (writing took 56.004531145095825 seconds)\n",
            "| epoch 003:   8% 142/1710 [01:20<14:55,  1.75it/s, loss=10.689, nll_loss=9.876, ppl=939.74, wps=12492, ups=2, wpb=6972.965, bsz=253.859, num_updates=3557, lr=0.00749848, gnorm=0.324, clip=0.000, oom=0.000, loss_scale=4.000, wall=2100, train_wall=1957]| WARNING: overflow detected, setting loss scale to: 2.0\n",
            "| epoch 003:   9% 148/1710 [01:24<14:36,  1.78it/s, loss=10.684, nll_loss=9.870, ppl=935.95, wps=12445, ups=2, wpb=6998.381, bsz=255.293, num_updates=3562, lr=0.00749321, gnorm=0.322, clip=0.000, oom=0.000, loss_scale=2.000, wall=2103, train_wall=1960]| WARNING: overflow detected, setting loss scale to: 1.0\n",
            "| epoch 003:  22% 383/1710 [03:34<12:40,  1.75it/s, loss=10.689, nll_loss=9.876, ppl=939.52, wps=12471, ups=2, wpb=6964.958, bsz=272.336, num_updates=3796, lr=0.00725858, gnorm=0.404, clip=0.031, oom=0.000, loss_scale=1.000, wall=2233, train_wall=2089]| WARNING: overflow detected, setting loss scale to: 0.5\n",
            "| epoch 003:  22% 384/1710 [03:34<12:20,  1.79it/s, loss=10.689, nll_loss=9.876, ppl=939.52, wps=12471, ups=2, wpb=6964.958, bsz=272.336, num_updates=3796, lr=0.00725858, gnorm=0.404, clip=0.031, oom=0.000, loss_scale=1.000, wall=2233, train_wall=2089]| WARNING: overflow detected, setting loss scale to: 0.25\n",
            "| epoch 003:  23% 387/1710 [03:36<11:27,  1.92it/s, loss=10.689, nll_loss=9.876, ppl=939.47, wps=12402, ups=2, wpb=6957.256, bsz=273.211, num_updates=3798, lr=0.00725667, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.250, wall=2235, train_wall=2091]| WARNING: overflow detected, setting loss scale to: 0.125\n",
            "| epoch 003:  23% 388/1710 [03:37<11:56,  1.85it/s, loss=10.689, nll_loss=9.876, ppl=939.47, wps=12402, ups=2, wpb=6957.256, bsz=273.211, num_updates=3798, lr=0.00725667, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.250, wall=2235, train_wall=2091]| WARNING: overflow detected, setting loss scale to: 0.0625\n",
            "| epoch 003:  23% 390/1710 [03:38<11:51,  1.86it/s, loss=10.689, nll_loss=9.875, ppl=939.32, wps=12343, ups=2, wpb=6959.430, bsz=273.104, num_updates=3799, lr=0.00725572, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.062, wall=2237, train_wall=2093]| WARNING: overflow detected, setting loss scale to: 0.03125\n",
            "| epoch 003:  23% 391/1710 [03:38<11:42,  1.88it/s, loss=10.689, nll_loss=9.875, ppl=939.32, wps=12343, ups=2, wpb=6959.430, bsz=273.104, num_updates=3799, lr=0.00725572, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.062, wall=2237, train_wall=2093]| WARNING: overflow detected, setting loss scale to: 0.015625\n",
            "| epoch 003:  23% 393/1710 [03:39<11:56,  1.84it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]| WARNING: overflow detected, setting loss scale to: 0.0078125\n",
            "| epoch 003:  23% 394/1710 [03:40<11:23,  1.93it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]| WARNING: overflow detected, setting loss scale to: 0.00390625\n",
            "| epoch 003:  23% 395/1710 [03:40<11:26,  1.92it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]| WARNING: overflow detected, setting loss scale to: 0.001953125\n",
            "| epoch 003:  23% 396/1710 [03:41<11:31,  1.90it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]| WARNING: overflow detected, setting loss scale to: 0.0009765625\n",
            "| epoch 003:  23% 397/1710 [03:41<11:28,  1.91it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]| WARNING: overflow detected, setting loss scale to: 0.00048828125\n",
            "| epoch 003:  23% 398/1710 [03:42<11:47,  1.86it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]| WARNING: overflow detected, setting loss scale to: 0.000244140625\n",
            "| epoch 003:  23% 399/1710 [03:42<11:51,  1.84it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]| WARNING: overflow detected, setting loss scale to: 0.0001220703125\n",
            "| epoch 003:  23% 400/1710 [03:43<11:28,  1.90it/s, loss=10.689, nll_loss=9.876, ppl=939.49, wps=12285, ups=2, wpb=6960.548, bsz=273.122, num_updates=3800, lr=0.00725476, gnorm=0.405, clip=0.031, oom=0.000, loss_scale=0.016, wall=2238, train_wall=2094]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 333, in cli_main\n",
            "    main(args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 86, in main\n",
            "    train(args, trainer, task, epoch_itr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 127, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/trainer.py\", line 433, in train_step\n",
            "    grad_norm = self.optimizer.clip_grad_norm(self.args.clip_norm)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/optim/fp16_optimizer.py\", line 146, in clip_grad_norm\n",
            "    ).format(self.min_loss_scale))\n",
            "FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPwRIrwU-0PO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "512d9618-0ee4-4300-b7a4-d8d2ef310e1a"
      },
      "source": [
        "! fairseq-train 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin' \\\n",
        "  --save-dir 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4' \\\n",
        "  --fp16 \\\n",
        "  --arch transformer --share-decoder-input-output-embed \\\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "  --optimizer adam --clip-norm 1.0 \\\n",
        "  --lr 1e-4 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
        "  --weight-decay 1e-4 \\\n",
        "  --dropout 0.2 \\\n",
        "  --max-epoch 5 \\\n",
        "  --max-tokens 8000 \\\n",
        "  --tensorboard-logdir 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_runs_-4'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=1.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=5, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=8000, max_tokens_valid=8000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_runs_-4', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=-1, warmup_updates=2000, weight_decay=0.0001)\n",
            "| [ja] dictionary: 70272 types\n",
            "| [en] dictionary: 59088 types\n",
            "| loaded 1166 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/valid.ja-en.ja\n",
            "| loaded 1166 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/valid.ja-en.en\n",
            "| drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin valid ja-en 1166 examples\n",
            "TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): Embedding(70272, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(59088, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "| model transformer, criterion LabelSmoothedCrossEntropyCriterion\n",
            "| num. model params: 110370816 (num. trained: 110370816)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = 8000 and max sentences per GPU = None\n",
            "| no existing checkpoint found drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 440288 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/train.ja-en.ja\n",
            "| loaded 440288 examples from: drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin/train.ja-en.en\n",
            "| drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin train ja-en 440288 examples\n",
            "| WARNING: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster\n",
            "| epoch 001:   0% 0/1710 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 64.0\n",
            "| epoch 001:   0% 1/1710 [00:00<24:56,  1.14it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "| epoch 001:   0% 2/1710 [00:01<21:48,  1.31it/s, loss=16.482, nll_loss=16.472, ppl=90908.8, wps=1411, ups=0, wpb=6646.000, bsz=80.000, num_updates=1, lr=5e-08, gnorm=7.514, clip=1.000, oom=0.000, loss_scale=64.000, wall=5, train_wall=1]| WARNING: overflow detected, setting loss scale to: 32.0\n",
            "| epoch 001:  11% 192/1710 [01:45<13:55,  1.82it/s, loss=14.309, nll_loss=14.054, ppl=17007, wps=11870, ups=2, wpb=6815.979, bsz=239.958, num_updates=190, lr=9.5e-06, gnorm=3.936, clip=1.000, oom=0.000, loss_scale=32.000, wall=109, train_wall=104]   | WARNING: overflow detected, setting loss scale to: 16.0\n",
            "| epoch 001:  18% 314/1710 [02:53<12:42,  1.83it/s, loss=13.528, nll_loss=13.184, ppl=9304.77, wps=11964, ups=2, wpb=6801.048, bsz=242.727, num_updates=311, lr=1.555e-05, gnorm=3.259, clip=1.000, oom=0.000, loss_scale=16.000, wall=177, train_wall=171]| WARNING: overflow detected, setting loss scale to: 8.0\n",
            "| epoch 001 | loss 10.329 | nll_loss 9.505 | ppl 726.58 | wps 12469 | ups 2 | wpb 6969.173 | bsz 254.907 | num_updates 1706 | lr 8.53e-05 | gnorm 2.136 | clip 0.961 | oom 0.000 | loss_scale 8.000 | wall 954 | train_wall 941\n",
            "| epoch 001 | valid on 'valid' subset | loss 8.789 | nll_loss 7.678 | ppl 204.81 | num_updates 1706\n",
            "| saved checkpoint drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4/checkpoint1.pt (epoch 1 @ 1706 updates) (writing took 56.47000789642334 seconds)\n",
            "| epoch 002 | loss 8.164 | nll_loss 6.995 | ppl 127.53 | wps 12547 | ups 2 | wpb 6970.249 | bsz 257.478 | num_updates 3416 | lr 7.65167e-05 | gnorm 1.386 | clip 0.767 | oom 0.000 | loss_scale 8.000 | wall 1961 | train_wall 1881\n",
            "| epoch 002 | valid on 'valid' subset | loss 7.805 | nll_loss 6.548 | ppl 93.56 | num_updates 3416 | best_loss 7.8055\n",
            "| saved checkpoint drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4/checkpoint2.pt (epoch 2 @ 3416 updates) (writing took 55.75609374046326 seconds)\n",
            "| epoch 003 | loss 7.500 | nll_loss 6.223 | ppl 74.69 | wps 12565 | ups 2 | wpb 6970.249 | bsz 257.478 | num_updates 5126 | lr 6.24634e-05 | gnorm 1.208 | clip 0.579 | oom 0.000 | loss_scale 8.000 | wall 2967 | train_wall 2821\n",
            "| epoch 003 | valid on 'valid' subset | loss 7.424 | nll_loss 6.101 | ppl 68.66 | num_updates 5126 | best_loss 7.42414\n",
            "| saved checkpoint drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4/checkpoint3.pt (epoch 3 @ 5126 updates) (writing took 55.933064222335815 seconds)\n",
            "| epoch 004 | loss 7.160 | nll_loss 5.825 | ppl 56.69 | wps 12570 | ups 2 | wpb 6970.249 | bsz 257.478 | num_updates 6836 | lr 5.40896e-05 | gnorm 1.145 | clip 0.489 | oom 0.000 | loss_scale 8.000 | wall 3973 | train_wall 3760\n",
            "| epoch 004 | valid on 'valid' subset | loss 7.186 | nll_loss 5.815 | ppl 56.31 | num_updates 6836 | best_loss 7.18634\n",
            "| saved checkpoint drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4/checkpoint4.pt (epoch 4 @ 6836 updates) (writing took 59.34161138534546 seconds)\n",
            "| epoch 005 | loss 6.928 | nll_loss 5.552 | ppl 46.92 | wps 12575 | ups 2 | wpb 6970.249 | bsz 257.478 | num_updates 8546 | lr 4.83764e-05 | gnorm 1.144 | clip 0.499 | oom 0.000 | loss_scale 8.000 | wall 4981 | train_wall 4699\n",
            "| epoch 005 | valid on 'valid' subset | loss 7.004 | nll_loss 5.598 | ppl 48.44 | num_updates 8546 | best_loss 7.00433\n",
            "| saved checkpoint drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4/checkpoint5.pt (epoch 5 @ 8546 updates) (writing took 55.2529616355896 seconds)\n",
            "| done training in 5034.4 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVmt_cSV-0kl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "731c3ba5-f742-40b5-d87f-e61c57f8163b"
      },
      "source": [
        "!fairseq-interactive --path 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-2/checkpoint2.pt' 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin' < 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.ja' | grep '^H' | cut -f 3 > 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-2.txt'\n",
        "!fairseq-interactive --path 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/96_result_checkpoints/checkpoint5.pt' 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin' < 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.ja' | grep '^H' | cut -f 3 > 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-3.txt'\n",
        "!fairseq-interactive --path 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_checkpoints_-4/checkpoint5.pt' 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/91_result_data-bin' < 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.ja' | grep '^H' | cut -f 3 > 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-4.txt'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAAKNG-VVzVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "80f30a36-2179-4900-defc-65a69464668a"
      },
      "source": [
        "!fairseq-score --sys 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-2.txt' --ref 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.en'\n",
        "!fairseq-score --sys 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-3.txt' --ref 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.en'\n",
        "!fairseq-score --sys 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-4.txt' --ref 'drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.en'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=False, order=4, ref='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.en', sacrebleu=False, sentence_bleu=False, sys='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-2.txt')\n",
            "BLEU4 = 0.00, 50.7/0.0/0.0/0.0 (BP=0.000, ratio=0.044, syslen=1160, reflen=26584)\n",
            "Namespace(ignore_case=False, order=4, ref='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.en', sacrebleu=False, sentence_bleu=False, sys='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-3.txt')\n",
            "BLEU4 = 2.39, 32.3/6.5/2.2/1.1 (BP=0.505, ratio=0.594, syslen=15798, reflen=26584)\n",
            "Namespace(ignore_case=False, order=4, ref='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/90_result_test.en', sacrebleu=False, sentence_bleu=False, sys='drive/My Drive/Colab Notebooks/100_knocks/chapter_10/results/97_result_output_-4.txt')\n",
            "BLEU4 = 5.42, 28.6/7.9/3.0/1.3 (BP=1.000, ratio=1.131, syslen=30073, reflen=26584)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NunDSdV4VzfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}