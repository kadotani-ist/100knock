{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../chapter_7/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = F.relu(self.i2h(combined))\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_id_dic(fname):\n",
    "    id_dic = {}\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        for i, line in enumerate(file, 1):\n",
    "            line = line.rstrip('\\n')\n",
    "            id_dic[line] = i\n",
    "            \n",
    "    return id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2id(sentence, id_dic):\n",
    "    import re\n",
    "    import snowballstemmer\n",
    "    \n",
    "    words_id = []\n",
    "    \n",
    "    # 文字種の統一\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 数字の置き換え\n",
    "    sentence = re.sub(r'[0-9]+', '0', sentence)\n",
    "    \n",
    "    # '-'を' 'に変換\n",
    "    sentence = sentence.replace('-', ' ')\n",
    "    \n",
    "    words = sentence.split()\n",
    "    \n",
    "    # ステミング処理\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    words2 = [stemmer.stemWord(word) for word in words]\n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        if word in id_dic.keys():\n",
    "            words_id.append(id_dic[word])\n",
    "        else:\n",
    "            words_id.append(0)\n",
    "            \n",
    "    return words_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2vec(words_id, id_dic):\n",
    "    words_vec = []\n",
    "    \n",
    "    for id in words_id:\n",
    "        word_list = [k for k, v in id_dic.items() if v == id]\n",
    "        if len(word_list) > 0:\n",
    "            try:\n",
    "                words_vec.append(torch.from_numpy(model[word_list[0]]).view(1,-1))\n",
    "            except KeyError:\n",
    "                words_vec.append(torch.zeros(1, 300))\n",
    "        else:\n",
    "            words_vec.append(torch.zeros(1, 300))\n",
    "    \n",
    "    return torch.cat(words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(fname, id_dic):\n",
    "    with open(fname) as file:\n",
    "        lines = file.readlines()\n",
    "        lines_vec = []\n",
    "        labels = np.zeros([len(lines), 1])\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            \n",
    "            line = line.rstrip('\\n')\n",
    "            category = line.strip('\\t')[0]\n",
    "            title = line.split('\\t')[1]\n",
    "            \n",
    "            # lines_vecの処理\n",
    "            words_id = words2id(title, id_dic)\n",
    "            words_vec = id2vec(words_id, id_dic)\n",
    "            lines_vec.append(words_vec)\n",
    "            \n",
    "            # labelsの処理\n",
    "            if category == 'b':\n",
    "                labels[i] = 0\n",
    "            elif category == 't':\n",
    "                labels[i] = 1\n",
    "            elif category == 'e':\n",
    "                labels[i] = 2\n",
    "            elif category == 'm':\n",
    "                labels[i] = 3\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "                \n",
    "    return lines_vec, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, criterion, optimizer, batch_size, words_vec_batch, label_batch):\n",
    "    hidden = rnn.initHidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    hidden = hidden.detach()\n",
    "    \n",
    "    for i in range(len(words_vec_batch)):\n",
    "        output, hidden = rnn(words_vec_batch[i], hidden)\n",
    "    loss = criterion(output, label_batch)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_accyracy(rnn, criterion, lines_vec, labels):\n",
    "    words_vec_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for words_vec, label in zip(lines_vec, labels):\n",
    "        words_vec_list.append(words_vec)\n",
    "        label_list.append(label)\n",
    "    words_vec_all, label_all = batch_process(words_vec_list, label_list)\n",
    "    \n",
    "    hidden = rnn.initHidden(len(lines_vec))\n",
    "    \n",
    "    for i in range(len(words_vec_all)):\n",
    "        output, hidden = rnn(words_vec_all[i], hidden)\n",
    "    loss = criterion(output, label_all)\n",
    "    pre_labels = torch.max(output, 1)[1]\n",
    "    accuracy = (pre_labels == label_all).sum().item() / len(labels)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_accyracy(rnn, criterion, lines_vec, labels):\n",
    "    running_loss = 0\n",
    "    correct_count = 0\n",
    "    \n",
    "    for words_vec, label in zip(lines_vec, labels):\n",
    "        words_vec_batch, label_batch = batch_process([words_vec], [label])\n",
    "        hidden = rnn.initHidden(1)\n",
    "        \n",
    "        for i in range(len(words_vec_batch)):\n",
    "            output, hidden = rnn(words_vec_batch[i], hidden)\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss\n",
    "        pre_label = torch.max(output, 1)[1]\n",
    "        if pre_label == label:\n",
    "            correct_count += 1\n",
    "            \n",
    "    return running_loss / len(labels), correct_count / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(words_vec_list, label_list):\n",
    "    # words_vec_batchの処理\n",
    "    len_max = 0\n",
    "    for words_vec in words_vec_list:\n",
    "        if words_vec.size()[0] > len_max:\n",
    "            len_max = words_vec.size()[0]\n",
    "    words_vec_batch = torch.zeros(len_max, len(words_vec_list), 300)\n",
    "    for i in range(len(words_vec_list)):\n",
    "        for j in range(len(words_vec_list[i])):\n",
    "            words_vec_batch[len_max - len(words_vec_list[i]) + j, i, :] = words_vec_list[i][j]\n",
    "        \n",
    "    # label_batchの処理\n",
    "    label_batch = torch.zeros(len(label_list)).long()\n",
    "    for i, label in enumerate(label_list):\n",
    "        label_batch[i] = label\n",
    "        \n",
    "    return words_vec_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 訓練データ上の損失: 1.0445014238357544\t訓練データ上の正解率: 0.591743119266055\t評価データ上の損失: 1.0557955503463745\t評価データ上の正解率: 0.5899550224887556\n",
      "[2] 訓練データ上の損失: 0.9616107940673828\t訓練データ上の正解率: 0.6486612993821381\t評価データ上の損失: 0.9791384935379028\t評価データ上の正解率: 0.643928035982009\n",
      "[3] 訓練データ上の損失: 0.7388445734977722\t訓練データ上の正解率: 0.735349185545778\t評価データ上の損失: 0.7333962321281433\t評価データ上の正解率: 0.739880059970015\n",
      "[4] 訓練データ上の損失: 0.5770920515060425\t訓練データ上の正解率: 0.780939898895338\t評価データ上の損失: 0.5921319723129272\t評価データ上の正解率: 0.7683658170914542\n",
      "[5] 訓練データ上の損失: 0.5535715222358704\t訓練データ上の正解率: 0.7879610559820258\t評価データ上の損失: 0.580390989780426\t評価データ上の正解率: 0.7758620689655172\n",
      "[6] 訓練データ上の損失: 0.5365055799484253\t訓練データ上の正解率: 0.7932035199400861\t評価データ上の損失: 0.5726305246353149\t評価データ上の正解率: 0.7766116941529235\n",
      "[7] 訓練データ上の損失: 0.5223622918128967\t訓練データ上の正解率: 0.799850215315484\t評価データ上の損失: 0.5666617751121521\t評価データ上の正解率: 0.7781109445277361\n",
      "[8] 訓練データ上の損失: 0.5100193023681641\t訓練データ上の正解率: 0.8044373712787868\t評価データ上の損失: 0.561617910861969\t評価データ上の正解率: 0.7818590704647677\n",
      "[9] 訓練データ上の損失: 0.4947988986968994\t訓練データ上の正解率: 0.8120202209324097\t評価データ上の損失: 0.5533351898193359\t評価データ上の正解率: 0.7848575712143928\n",
      "[10] 訓練データ上の損失: 0.48323068022727966\t訓練データ上の正解率: 0.8148286837670848\t評価データ上の損失: 0.5485417246818542\t評価データ上の正解率: 0.7908545727136432\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "rnn = RNN(300, 50, 4) # RNN(d_w, d_h, L)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=pow(10, -2)) # lr: 検証の結果、10エポック目で最も精度が高かった数値\n",
    "\n",
    "id_dic = make_id_dic('../chapter_6/train.feature.txt')\n",
    "lines_vec, labels = make_dataset('../chapter_6/train.txt', id_dic)\n",
    "lines_vec_test, labels_test = make_dataset('../chapter_6/test.txt', id_dic)\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(10):\n",
    "    words_vec_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, dataset in enumerate(zip(lines_vec, labels), 1):\n",
    "        words_vec_list.append(dataset[0])\n",
    "        label_list.append(dataset[1])\n",
    "        \n",
    "        if i % batch_size == 0 or i == len(lines_vec):\n",
    "            words_vec_batch, label_batch = batch_process(words_vec_list, label_list)\n",
    "            train(rnn, criterion, optimizer, len(words_vec_list), words_vec_batch, label_batch)\n",
    "            words_vec_list = []\n",
    "            label_list = []\n",
    "            \n",
    "    loss_train, accuracy_train = get_loss_and_accyracy(rnn, criterion, lines_vec, labels)\n",
    "    loss_test, accuracy_test = get_loss_and_accyracy(rnn, criterion, lines_vec_test, labels_test)\n",
    "    print('[{0}] 訓練データ上の損失: {1}\\t訓練データ上の正解率: {2}\\t評価データ上の損失: {3}\\t評価データ上の正解率: {4}'.format(epoch + 1, loss_train, accuracy_train, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
