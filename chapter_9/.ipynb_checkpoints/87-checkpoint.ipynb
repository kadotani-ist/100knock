{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../chapter_7/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, d_h):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 50, (3, 300), padding=(1, 0))\n",
    "        self.fc1 = nn.Linear(d_h, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        words_len = x.size(2)\n",
    "        \n",
    "        x =  F.relu(self.conv1(x)).squeeze(3)\n",
    "        x = F.max_pool2d(x, (1, words_len)).squeeze(2)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_id_dic(fname):\n",
    "    id_dic = {}\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        for i, line in enumerate(file, 1):\n",
    "            line = line.rstrip('\\n')\n",
    "            id_dic[line] = i\n",
    "            \n",
    "    return id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2id(sentence, id_dic):\n",
    "    import re\n",
    "    import snowballstemmer\n",
    "    \n",
    "    words_id = []\n",
    "    \n",
    "    # 文字種の統一\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 数字の置き換え\n",
    "    sentence = re.sub(r'[0-9]+', '0', sentence)\n",
    "    \n",
    "    # '-'を' 'に変換\n",
    "    sentence = sentence.replace('-', ' ')\n",
    "    \n",
    "    words = sentence.split()\n",
    "    \n",
    "    # ステミング処理\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    words2 = [stemmer.stemWord(word) for word in words]\n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        if word in id_dic.keys():\n",
    "            words_id.append(id_dic[word])\n",
    "        else:\n",
    "            words_id.append(0)\n",
    "            \n",
    "    return words_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2vec(words_id, id_dic):\n",
    "    words_vec = []\n",
    "    \n",
    "    for id in words_id:\n",
    "        word_list = [k for k, v in id_dic.items() if v == id]\n",
    "        if len(word_list) > 0:\n",
    "            try:\n",
    "                words_vec.append(torch.from_numpy(model[word_list[0]]).view(1,-1))\n",
    "            except KeyError:\n",
    "                words_vec.append(torch.zeros(1, 300))\n",
    "        else:\n",
    "            words_vec.append(torch.zeros(1, 300))\n",
    "    \n",
    "    return torch.cat(words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(fname, id_dic):\n",
    "    with open(fname) as file:\n",
    "        lines = file.readlines()\n",
    "        lines_vec = []\n",
    "        labels = np.zeros([len(lines), 1])\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            \n",
    "            line = line.rstrip('\\n')\n",
    "            category = line.strip('\\t')[0]\n",
    "            title = line.split('\\t')[1]\n",
    "            \n",
    "            # lines_vecの処理\n",
    "            words_id = words2id(title, id_dic)\n",
    "            words_vec = id2vec(words_id, id_dic)\n",
    "            lines_vec.append(words_vec)\n",
    "            \n",
    "            # labelsの処理\n",
    "            if category == 'b':\n",
    "                labels[i] = 0\n",
    "            elif category == 't':\n",
    "                labels[i] = 1\n",
    "            elif category == 'e':\n",
    "                labels[i] = 2\n",
    "            elif category == 'm':\n",
    "                labels[i] = 3\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "                \n",
    "    return lines_vec, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cnn, criterion, optimizer, words_vec, label):\n",
    "    cnn.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = cnn(words_vec.unsqueeze(0).unsqueeze(0))\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_accyracy(cnn, criterion, lines_vec, labels):\n",
    "    running_loss = 0\n",
    "    correct_count = 0\n",
    "    \n",
    "    for words_vec, label in zip(lines_vec, labels):\n",
    "        output = cnn(words_vec.unsqueeze(0).unsqueeze(0))\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss\n",
    "        pre_label = torch.max(output, 1)[1]\n",
    "        if pre_label == label:\n",
    "            correct_count += 1\n",
    "            \n",
    "    return running_loss / len(labels), correct_count / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 訓練データ上の損失: 0.3290466368198395\t訓練データ上の正解率: 0.8892529488859764\t評価データ上の損失: 0.41012105345726013\t評価データ上の正解率: 0.8583208395802099\n",
      "[2] 訓練データ上の損失: 0.22339335083961487\t訓練データ上の正解率: 0.9313798914061038\t評価データ上の損失: 0.36543741822242737\t評価データ上の正解率: 0.8770614692653673\n",
      "[3] 訓練データ上の損失: 0.15620648860931396\t訓練データ上の正解率: 0.9569369032016476\t評価データ上の損失: 0.3525507152080536\t評価データ上の正解率: 0.8770614692653673\n",
      "[4] 訓練データ上の損失: 0.10955225676298141\t訓練データ上の正解率: 0.9722898333645384\t評価データ上の損失: 0.3574272394180298\t評価データ上の正解率: 0.8800599700149925\n",
      "[5] 訓練データ上の損失: 0.07729192823171616\t訓練データ上の正解率: 0.9828683767084816\t評価データ上の損失: 0.3787167966365814\t評価データ上の正解率: 0.883808095952024\n",
      "[6] 訓練データ上の損失: 0.05431031808257103\t訓練データ上の正解率: 0.989421456656057\t評価データ上の損失: 0.3995789587497711\t評価データ上の正解率: 0.8778110944527736\n",
      "[7] 訓練データ上の損失: 0.0399080254137516\t訓練データ上の正解率: 0.9938213817637147\t評価データ上の損失: 0.41736680269241333\t評価データ上の正解率: 0.8748125937031485\n",
      "[8] 訓練データ上の損失: 0.0319071039557457\t訓練データ上の正解率: 0.9951319977532297\t評価データ上の損失: 0.4317984879016876\t評価データ上の正解率: 0.8770614692653673\n",
      "[9] 訓練データ上の損失: 0.028202321380376816\t訓練データ上の正解率: 0.9959745366036323\t評価データ上の損失: 0.44280868768692017\t評価データ上の正解率: 0.876311844077961\n",
      "[10] 訓練データ上の損失: 0.023763738572597504\t訓練データ上の正解率: 0.9962553828870998\t評価データ上の損失: 0.45465224981307983\t評価データ上の正解率: 0.8740629685157422\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "d_h = 50 # d_h: 畳み込み演算後の各時刻のベクトルの次元数\n",
    "cnn = Net(d_h)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=pow(10, -2)) # lr: 検証の結果、10エポック目で最も精度が高かった数値\n",
    "\n",
    "id_dic = make_id_dic('../chapter_6/train.feature.txt')\n",
    "lines_vec, labels = make_dataset('../chapter_6/train.txt', id_dic)\n",
    "lines_vec_test, labels_test = make_dataset('../chapter_6/test.txt', id_dic)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for words_vec, label in zip(lines_vec, labels):\n",
    "        train(cnn, criterion, optimizer, words_vec, label)\n",
    "    loss_train, accuracy_train = get_loss_and_accyracy(cnn, criterion, lines_vec, labels)\n",
    "    loss_test, accuracy_test = get_loss_and_accyracy(cnn, criterion, lines_vec_test, labels_test)\n",
    "    print('[{0}] 訓練データ上の損失: {1}\\t訓練データ上の正解率: {2}\\t評価データ上の損失: {3}\\t評価データ上の正解率: {4}'.format(epoch + 1, loss_train, accuracy_train, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
