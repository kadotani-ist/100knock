{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tGU8nSnjxh6u",
    "outputId": "3b9e9b3f-27f9-4f3d-eb60-d8591ea0d675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at ./drive; to attempt to forcibly remount, call drive.mount(\"./drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('./drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdpULQwoyUK7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFFfcyCmy6VZ"
   },
   "outputs": [],
   "source": [
    "def make_id_dic(fname):\n",
    "    id_dic = {}\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        for i, line in enumerate(file, 1):\n",
    "            line = line.rstrip('\\n')\n",
    "            id_dic[line] = i\n",
    "            \n",
    "    return id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdQYUYVj0Eos"
   },
   "outputs": [],
   "source": [
    "def words2id(sentence, id_dic):\n",
    "    import re\n",
    "    import snowballstemmer\n",
    "    \n",
    "    words_id = []\n",
    "    \n",
    "    # 文字種の統一\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 数字の置き換え\n",
    "    sentence = re.sub(r'[0-9]+', '0', sentence)\n",
    "    \n",
    "    # '-'を' 'に変換\n",
    "    sentence = sentence.replace('-', ' ')\n",
    "    \n",
    "    words = sentence.split()\n",
    "    \n",
    "    # ステミング処理\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    words2 = [stemmer.stemWord(word) for word in words]\n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        if word in id_dic.keys():\n",
    "            words_id.append(id_dic[word])\n",
    "        else:\n",
    "            words_id.append(0)\n",
    "            \n",
    "    return words_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q83reJ4h0Ny2"
   },
   "outputs": [],
   "source": [
    "def id2vec(words_id, embeds):\n",
    "    words_vec = embeds(torch.Tensor(words_id).long())\n",
    "    \n",
    "    return words_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYXfPxY00UVg"
   },
   "outputs": [],
   "source": [
    "def make_dataset(fname, id_dic, embeds):\n",
    "    with open(fname) as file:\n",
    "        lines = file.readlines()\n",
    "        lines_vec = []\n",
    "        labels = np.zeros([len(lines), 1])\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            \n",
    "            line = line.rstrip('\\n')\n",
    "            category = line.strip('\\t')[0]\n",
    "            title = line.split('\\t')[1]\n",
    "            \n",
    "            # lines_vecの処理\n",
    "            words_id = words2id(title, id_dic)\n",
    "            words_vec = id2vec(words_id, embeds)\n",
    "            lines_vec.append(words_vec)\n",
    "            \n",
    "            # labelsの処理\n",
    "            if category == 'b':\n",
    "                labels[i] = 0\n",
    "            elif category == 't':\n",
    "                labels[i] = 1\n",
    "            elif category == 'e':\n",
    "                labels[i] = 2\n",
    "            elif category == 'm':\n",
    "                labels[i] = 3\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "                \n",
    "    return lines_vec, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INH7-bpY42G8"
   },
   "outputs": [],
   "source": [
    "def train(rnn, criterion, optimizer, batch_size, words_vec_batch, label_batch):\n",
    "    hidden = rnn.initHidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    hidden = hidden.detach()\n",
    "    \n",
    "    for i in range(len(words_vec_batch)):\n",
    "        output, hidden = rnn(words_vec_batch[i].to(device), hidden.to(device))\n",
    "    loss = criterion(output, label_batch.to(device))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTBw-IAh4Pja"
   },
   "outputs": [],
   "source": [
    "def get_loss_and_accyracy(rnn, criterion, lines_vec, labels):\n",
    "    running_loss = 0\n",
    "    correct_count = 0\n",
    "    words_vec_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for words_vec, label in zip(lines_vec, labels):\n",
    "        words_vec_list.append(words_vec)\n",
    "        label_list.append(label)\n",
    "    words_vec_all, label_all = batch_process(words_vec_list, label_list)\n",
    "    \n",
    "    hidden = rnn.initHidden(len(lines_vec))\n",
    "    \n",
    "    for i in range(len(words_vec_all)):\n",
    "        output, hidden = rnn(words_vec_all[i].to(device), hidden.to(device))\n",
    "    loss = criterion(output, label_all.to(device))\n",
    "    pre_labels = torch.max(output, 1)[1]\n",
    "    accuracy = (pre_labels == label_all.to(device)).sum().item() / len(labels)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0tSYT7r4R7C"
   },
   "outputs": [],
   "source": [
    "def batch_process(words_vec_list, label_list):\n",
    "    # words_vec_batchの処理\n",
    "    len_max = 0\n",
    "    for words_vec in words_vec_list:\n",
    "        if words_vec.size()[0] > len_max:\n",
    "            len_max = words_vec.size()[0]\n",
    "    words_vec_batch = torch.zeros(len_max, len(words_vec_list), 300)\n",
    "    for i in range(len(words_vec_list)):\n",
    "        for j in range(len(words_vec_list[i])):\n",
    "            words_vec_batch[len_max - len(words_vec_list[i]) + j, i, :] = words_vec_list[i][j]\n",
    "        \n",
    "    # label_batchの処理\n",
    "    label_batch = torch.zeros(len(label_list)).long()\n",
    "    for i, label in enumerate(label_list):\n",
    "        label_batch[i] = label\n",
    "        \n",
    "    return words_vec_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "DygKB4NTAzBn",
    "outputId": "6785d376-247e-437a-a0c2-97f83ef194e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "\n",
      "[1] 訓練データ上の損失: 1.17604660987854\t訓練データ上の正解率: 0.508519003931848\t評価データ上の損失: 1.194509506225586\t評価データ上の正解率: 0.507496251874063\n",
      "[2] 訓練データ上の損失: 1.094875454902649\t訓練データ上の正解率: 0.5619734132184984\t評価データ上の損失: 1.1291295289993286\t評価データ上の正解率: 0.5449775112443778\n",
      "[3] 訓練データ上の損失: 1.050013542175293\t訓練データ上の正解率: 0.5837858079011421\t評価データ上の損失: 1.0972168445587158\t評価データ上の正解率: 0.5584707646176912\n",
      "[4] 訓練データ上の損失: 1.019877314567566\t訓練データ上の正解率: 0.5961430443737128\t評価データ上の損失: 1.0783015489578247\t評価データ上の正解率: 0.56071964017991\n",
      "[5] 訓練データ上の損失: 0.9968733787536621\t訓練データ上の正解率: 0.6043812020220932\t評価データ上の損失: 1.065365195274353\t評価データ上の正解率: 0.5749625187406296\n",
      "[6] 訓練データ上の損失: 0.9775402545928955\t訓練データ上の正解率: 0.6117768208200711\t評価データ上の損失: 1.0552279949188232\t評価データ上の正解率: 0.5787106446776612\n",
      "[7] 訓練データ上の損失: 0.9600287079811096\t訓練データ上の正解率: 0.6202022093240966\t評価データ上の損失: 1.0462276935577393\t評価データ上の正解率: 0.5854572713643178\n",
      "[8] 訓練データ上の損失: 0.9430308938026428\t訓練データ上の正解率: 0.6279722898333645\t評価データ上の損失: 1.0371863842010498\t評価データ上の正解率: 0.5937031484257871\n",
      "[9] 訓練データ上の損失: 0.9250552654266357\t訓練データ上の正解率: 0.6360232166261\t評価データ上の損失: 1.0266468524932861\t評価データ上の正解率: 0.6004497751124438\n",
      "[10] 訓練データ上の損失: 0.9031846523284912\t訓練データ上の正解率: 0.6453847594083505\t評価データ上の損失: 1.0114225149154663\t評価データ上の正解率: 0.6049475262368815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('{0}\\n'.format(device))\n",
    "\n",
    "rnn = RNN(300, 50, 4).to(device) # RNN(d_w, d_h, L)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=pow(10, -3)) # lr: 検証の結果、10エポック目で最も精度が高かった数値\n",
    "\n",
    "id_dic = make_id_dic('drive/My Drive/Colab Notebooks/100_knocks/chapter_6/train.feature.txt')\n",
    "embeds = nn.Embedding(len(id_dic)+1, 300)\n",
    "lines_vec, labels = make_dataset('drive/My Drive/Colab Notebooks/100_knocks/chapter_6/train.txt', id_dic, embeds)\n",
    "lines_vec_test, labels_test = make_dataset('drive/My Drive/Colab Notebooks/100_knocks/chapter_6/test.txt', id_dic, embeds)\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0\n",
    "    words_vec_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, dataset in enumerate(zip(lines_vec, labels), 1):\n",
    "        words_vec_list.append(dataset[0])\n",
    "        label_list.append(dataset[1])\n",
    "        \n",
    "        if i % batch_size == 0 or i == len(lines_vec):\n",
    "            words_vec_batch, label_batch = batch_process(words_vec_list, label_list)\n",
    "            loss = train(rnn, criterion, optimizer, len(words_vec_list), words_vec_batch, label_batch)\n",
    "            running_loss += loss\n",
    "            words_vec_list = []\n",
    "            label_list = []\n",
    "            \n",
    "    loss_train, accuracy_train = get_loss_and_accyracy(rnn, criterion, lines_vec, labels)\n",
    "    loss_test, accuracy_test = get_loss_and_accyracy(rnn, criterion, lines_vec_test, labels_test)\n",
    "    print('[{0}] 訓練データ上の損失: {1}\\t訓練データ上の正解率: {2}\\t評価データ上の損失: {3}\\t評価データ上の正解率: {4}'.format(epoch + 1, loss_train, accuracy_train, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zd5FvJyv3pIB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "83.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
