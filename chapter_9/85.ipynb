{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../chapter_7/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 順方向のRNN\n",
    "        self.i2fc = nn.Linear(input_size + hidden_size, 256)\n",
    "        self.fc2h = nn.Linear(256, hidden_size)\n",
    "        \n",
    "        # 逆方向のRNN\n",
    "        self.i_b2fc_b = nn.Linear(input_size + hidden_size, 256)\n",
    "        self.fc_b2h_b = nn.Linear(256, hidden_size)\n",
    "        \n",
    "        # output\n",
    "        self.h2o = nn.Linear(hidden_size + hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, input_b, hidden, hidden_b):\n",
    "        # 順方向のRNN\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        combined = F.relu(self.i2fc(combined))\n",
    "        hidden = F.relu(self.fc2h(combined))\n",
    "        \n",
    "        # 逆方向のRNN\n",
    "        combined_b = torch.cat((input_b, hidden_b), 1)\n",
    "        combined_b = F.relu(self.i_b2fc_b(combined_b))\n",
    "        hidden_b = F.relu(self.fc_b2h_b(combined_b))\n",
    "        \n",
    "        # output\n",
    "        combined_o = torch.cat((hidden, hidden_b), 1)\n",
    "        output = self.h2o(combined_o)\n",
    "        return output, hidden, hidden_b\n",
    "        \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_id_dic(fname):\n",
    "    id_dic = {}\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        for i, line in enumerate(file, 1):\n",
    "            line = line.rstrip('\\n')\n",
    "            id_dic[line] = i\n",
    "            \n",
    "    return id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2id(sentence, id_dic):\n",
    "    import re\n",
    "    import snowballstemmer\n",
    "    \n",
    "    words_id = []\n",
    "    \n",
    "    # 文字種の統一\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 数字の置き換え\n",
    "    sentence = re.sub(r'[0-9]+', '0', sentence)\n",
    "    \n",
    "    # '-'を' 'に変換\n",
    "    sentence = sentence.replace('-', ' ')\n",
    "    \n",
    "    words = sentence.split()\n",
    "    \n",
    "    # ステミング処理\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    words2 = [stemmer.stemWord(word) for word in words]\n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        if word in id_dic.keys():\n",
    "            words_id.append(id_dic[word])\n",
    "        else:\n",
    "            words_id.append(0)\n",
    "            \n",
    "    return words_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2vec(words_id, id_dic):\n",
    "    words_vec = []\n",
    "    \n",
    "    for id in words_id:\n",
    "        word_list = [k for k, v in id_dic.items() if v == id]\n",
    "        if len(word_list) > 0:\n",
    "            try:\n",
    "                words_vec.append(torch.from_numpy(model[word_list[0]]).view(1,-1))\n",
    "            except KeyError:\n",
    "                words_vec.append(torch.zeros(1, 300))\n",
    "        else:\n",
    "            words_vec.append(torch.zeros(1, 300))\n",
    "    \n",
    "    return torch.cat(words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(fname, id_dic):\n",
    "    with open(fname) as file:\n",
    "        lines = file.readlines()\n",
    "        lines_vec = []\n",
    "        labels = np.zeros([len(lines), 1])\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            \n",
    "            line = line.rstrip('\\n')\n",
    "            category = line.strip('\\t')[0]\n",
    "            title = line.split('\\t')[1]\n",
    "            \n",
    "            # lines_vecの処理\n",
    "            words_id = words2id(title, id_dic)\n",
    "            words_vec = id2vec(words_id, id_dic)\n",
    "            lines_vec.append(words_vec)\n",
    "            \n",
    "            # labelsの処理\n",
    "            if category == 'b':\n",
    "                labels[i] = 0\n",
    "            elif category == 't':\n",
    "                labels[i] = 1\n",
    "            elif category == 'e':\n",
    "                labels[i] = 2\n",
    "            elif category == 'm':\n",
    "                labels[i] = 3\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "                \n",
    "    return lines_vec, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, criterion, optimizer, batch_size, words_vec_batch, words_vec_batch_b, label_batch):\n",
    "    hidden, hidden_b = rnn.initHidden(batch_size), rnn.initHidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    hidden, hidden_b = hidden.detach(), hidden_b.detach()\n",
    "    \n",
    "    for i in range(len(words_vec_batch)):\n",
    "        output, hidden, hidden_b = rnn(words_vec_batch[i], words_vec_batch_b[i], hidden, hidden_b)\n",
    "    loss = criterion(output, label_batch)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_accyracy(rnn, criterion, lines_vec, labels):\n",
    "    running_loss = 0\n",
    "    correct_count = 0\n",
    "    \n",
    "    for words_vec, label in zip(lines_vec, labels):\n",
    "        words_vec_batch, words_vec_batch_b, label_batch = batch_process([words_vec], [label])\n",
    "        hidden, hidden_b = rnn.initHidden(1), rnn.initHidden(1)\n",
    "        \n",
    "        for i in range(len(words_vec_batch)):\n",
    "            output, hidden, hidden_b = rnn(words_vec_batch[i], words_vec_batch_b[i], hidden, hidden_b)\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss\n",
    "        pre_label = torch.max(output, 1)[1]\n",
    "        if pre_label == label:\n",
    "            correct_count += 1\n",
    "            \n",
    "    return running_loss / len(labels), correct_count / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(words_vec_list, label_list):\n",
    "    # words_vec_batchの処理\n",
    "    len_max = 0\n",
    "    for words_vec in words_vec_list:\n",
    "        if words_vec.size()[0] > len_max:\n",
    "            len_max = words_vec.size()[0]\n",
    "    words_vec_batch = torch.zeros(len_max, len(words_vec_list), 300)\n",
    "    for i in range(len(words_vec_list)):\n",
    "        for j in range(len(words_vec_list[i])):\n",
    "            words_vec_batch[len_max - len(words_vec_list[i]) + j, i, :] = words_vec_list[i][j]\n",
    "            \n",
    "    # words_vec_batch_bの処理\n",
    "    words_vec_batch_b = torch.zeros(len_max, len(words_vec_list), 300)\n",
    "    for i in range(len(words_vec_list)):\n",
    "        words_vec_batch_b[:, i, :] = words_vec_batch[:, len(words_vec_list) - 1 - i, :]\n",
    "    \n",
    "    # label_batchの処理\n",
    "    label_batch = torch.zeros(len(label_list)).long()\n",
    "    for i, label in enumerate(label_list):\n",
    "        label_batch[i] = label\n",
    "        \n",
    "    return words_vec_batch, words_vec_batch_b, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 訓練データ上の損失: 1.1401758193969727\t訓練データ上の正解率: 0.42389065718030333\t評価データ上の損失: 1.1531083583831787\t評価データ上の正解率: 0.4197901049475262\n",
      "[2] 訓練データ上の損失: 0.7806670069694519\t訓練データ上の正解率: 0.7427448043437559\t評価データ上の損失: 0.7939436435699463\t評価データ上の正解率: 0.7346326836581709\n",
      "[3] 訓練データ上の損失: 0.7390268445014954\t訓練データ上の正解率: 0.7535105785433439\t評価データ上の損失: 0.7515162229537964\t評価データ上の正解率: 0.7421289355322339\n",
      "[4] 訓練データ上の損失: 0.641130805015564\t訓練データ上の正解率: 0.7695188167009923\t評価データ上の損失: 0.6471962928771973\t評価データ上の正解率: 0.767616191904048\n",
      "[5] 訓練データ上の損失: 0.5836446285247803\t訓練データ上の正解率: 0.7790675903388878\t評価データ上の損失: 0.6026234030723572\t評価データ上の正解率: 0.7698650674662668\n",
      "[6] 訓練データ上の損失: 0.5642468929290771\t訓練データ上の正解率: 0.7802845909005804\t評価データ上の損失: 0.5968927145004272\t評価データ上の正解率: 0.7668665667166417\n",
      "[7] 訓練データ上の損失: 0.5588259696960449\t訓練データ上の正解率: 0.7823441303126756\t評価データ上の損失: 0.6023554801940918\t評価データ上の正解率: 0.7698650674662668\n",
      "[8] 訓練データ上の損失: 0.9192905426025391\t訓練データ上の正解率: 0.7454596517506085\t評価データ上の損失: 0.9426857829093933\t評価データ上の正解率: 0.7376311844077961\n",
      "[9] 訓練データ上の損失: 0.6713577508926392\t訓練データ上の正解率: 0.7647444298820446\t評価データ上の損失: 0.6996947526931763\t評価データ上の正解率: 0.7466266866566716\n",
      "[10] 訓練データ上の損失: 0.6434522271156311\t訓練データ上の正解率: 0.7674592772888972\t評価データ上の損失: 0.6709199547767639\t評価データ上の正解率: 0.7548725637181409\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "rnn = RNN(300, 50, 4) # RNN(d_w, d_h, L)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=pow(10, -2)) # lr: 検証の結果、10エポック目で最も精度が高かった数値\n",
    "\n",
    "id_dic = make_id_dic('../chapter_6/train.feature.txt')\n",
    "lines_vec, labels = make_dataset('../chapter_6/train.txt', id_dic)\n",
    "lines_vec_test, labels_test = make_dataset('../chapter_6/test.txt', id_dic)\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(10):\n",
    "    words_vec_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, dataset in enumerate(zip(lines_vec, labels), 1):\n",
    "        words_vec_list.append(dataset[0])\n",
    "        label_list.append(dataset[1])\n",
    "        \n",
    "        if i % batch_size == 0 or i == len(lines_vec):\n",
    "            words_vec_batch, words_vec_batch_b, label_batch = batch_process(words_vec_list, label_list)\n",
    "            train(rnn, criterion, optimizer, len(words_vec_list), words_vec_batch, words_vec_batch_b, label_batch)\n",
    "            words_vec_list = []\n",
    "            label_list = []\n",
    "            \n",
    "    loss_train, accuracy_train = get_loss_and_accyracy(rnn, criterion, lines_vec, labels)\n",
    "    loss_test, accuracy_test = get_loss_and_accyracy(rnn, criterion, lines_vec_test, labels_test)\n",
    "    print('[{0}] 訓練データ上の損失: {1}\\t訓練データ上の正解率: {2}\\t評価データ上の損失: {3}\\t評価データ上の正解率: {4}'.format(epoch + 1, loss_train, accuracy_train, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
