{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_dic(fname):\n",
    "    with open(fname) as file:\n",
    "        return {line.rstrip('\\n'): i for i, line in enumerate(file)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector_x(title, features_dic):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    import snowballstemmer\n",
    "    \n",
    "    vector_x = np.zeros([1, len(features_dic)])\n",
    "    \n",
    "    # 文字種の統一\n",
    "    title = title.lower()\n",
    "            \n",
    "    # 数字の置き換え -> 除去\n",
    "    title = re.sub(r'[0-9]+', '', title)\n",
    "    \n",
    "    # '-'を' 'に変換\n",
    "    title = title.replace('-', ' ')\n",
    "    \n",
    "    words = title.split()\n",
    "    \n",
    "    # ストップワードの除去\n",
    "    stop_words = stopwords.words('english')\n",
    "    words2 = [word for word in words if word not in stop_words]\n",
    "    words = words2       \n",
    "    \n",
    "    # ステミング処理\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    words2 = [stemmer.stemWord(word) for word in words]\n",
    "    words = words2\n",
    "    \n",
    "    # 記号の除去\n",
    "    words2 = [word for word in words if word.islower()]\n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        if word in features_dic.keys():\n",
    "            vector_x[0, features_dic[word]] = 1\n",
    "            \n",
    "    return vector_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(queue): # ソフトマックス関数\n",
    "    return np.exp(queue) / np.sum(np.exp(queue), axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predeict_category(vector_x, fname):\n",
    "    w_and_b = np.load(fname)\n",
    "    queue_w = w_and_b[:-1]\n",
    "    vector_b = w_and_b[-1]\n",
    "    phi = softmax(np.dot(vector_x, queue_w) + vector_b)\n",
    "    class_label = np.argmax(phi, axis = 1)\n",
    "    \n",
    "    if class_label == 0:\n",
    "        category = 'b'\n",
    "    elif class_label == 1:\n",
    "        category = 't'\n",
    "    elif class_label == 2:\n",
    "        category = 'e'\n",
    "    elif class_label == 3:\n",
    "        category = 'm'\n",
    "        \n",
    "    return category, phi[:,class_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter title: Disney Announce Plans For Two New Sequels: 'The Incredibles 2' and 'Cars 3'\n",
      "カテゴリ: e\t予測確率: [[0.9982954]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "title = input('enter title: ')\n",
    "features_dic = make_features_dic('train.feature.txt')\n",
    "vector_x = make_vector_x(title, features_dic)\n",
    "pred_category, probability = predeict_category(vector_x, 'results/52_result.npy')\n",
    "\n",
    "print('カテゴリ: {0}\\t予測確率: {1}'.format(pred_category, probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
