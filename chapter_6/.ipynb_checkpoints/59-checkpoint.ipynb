{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_dic(fname):\n",
    "    with open(fname) as file:\n",
    "        return {line.rstrip('\\n'): i for i, line in enumerate(file)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_queue(fname, features_dic):\n",
    "    with open(fname) as file:\n",
    "        lines = file.readlines()\n",
    "        queue_x = np.zeros([len(lines), len(features_dic)])\n",
    "        queue_y = np.zeros([len(lines), 4]) # One-hot表現: [b, t, e, m]\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            import re\n",
    "            from nltk.corpus import stopwords\n",
    "            import snowballstemmer\n",
    "            \n",
    "            line = line.rstrip('\\n')\n",
    "            category = line.strip('\\t')[0]\n",
    "            title = line.split('\\t')[1]\n",
    "            \n",
    "            # queue_xの処理\n",
    "            # 文字種の統一\n",
    "            title = title.lower()\n",
    "            \n",
    "            # 数字の置き換え -> 除去\n",
    "            title = re.sub(r'[0-9]+', '', title)\n",
    "            \n",
    "            # '-'を' 'に変換\n",
    "            title = title.replace('-', ' ')\n",
    "            \n",
    "            words = title.split()\n",
    "            \n",
    "            # ストップワードの除去\n",
    "            stop_words = stopwords.words('english')\n",
    "            words2 = [word for word in words if word not in stop_words]\n",
    "            words = words2\n",
    "            \n",
    "            # ステミング処理\n",
    "            stemmer = snowballstemmer.stemmer('english')\n",
    "            words2 = [stemmer.stemWord(word) for word in words]\n",
    "            words = words2\n",
    "            \n",
    "            # 記号の除去\n",
    "            words2 = [word for word in words if word.islower()]\n",
    "            words = words2\n",
    "            \n",
    "            for word in words:\n",
    "                if word in features_dic.keys():\n",
    "                    queue_x[i, features_dic[word]] = 1\n",
    "                    \n",
    "            # queue_yの処理\n",
    "            if category == 'b':\n",
    "                queue_y[i, 0] = 1\n",
    "            elif category == 't':\n",
    "                queue_y[i, 1] = 1\n",
    "            elif category == 'e':\n",
    "                queue_y[i, 2] = 1\n",
    "            elif category == 'm':\n",
    "                queue_y[i, 3] = 1\n",
    "                \n",
    "    return queue_x, queue_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(queue): # ソフトマックス関数\n",
    "    return np.exp(queue) / np.sum(np.exp(queue), axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(queue_x, queue_y, queue_w, vector_b, rp):\n",
    "    phi = softmax(np.dot(queue_x, queue_w) + vector_b)\n",
    "    \n",
    "    vector_l2_norm = np.zeros(len(queue_w))\n",
    "    for i in range(len(queue_w)):\n",
    "        vector_l2_norm[i] = np.sqrt(np.sum(np.square(queue_w[i])))\n",
    "    l2 = (rp / 2) * np.sum(vector_l2_norm) # L2正則化項\n",
    "    \n",
    "    closs_entropy = -np.sum(queue_y * np.log(phi)) +l2 # 交差エントロピー\n",
    "    \n",
    "    l2_dw = np.zeros(queue_w.shape) # L2正則化項のwについての偏微分\n",
    "    for i in range(len(queue_w)):\n",
    "        for j in range(4):\n",
    "            l2_dw = (rp / 2) * (queue_w[i][j] / vector_l2_norm[i])\n",
    "    \n",
    "    dw = -np.dot(queue_x.T, queue_y - phi) + l2_dw\n",
    "    db = -np.dot(np.ones([1, queue_y.shape[0]]), queue_y - phi)\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(queue_x, queue_y, queue_w, vector_b, lr, rp):\n",
    "    dw, db = calculate_gradient(queue_x, queue_y, queue_w, vector_b, rp)\n",
    "    queue_w -= lr * dw\n",
    "    vector_b -= lr * db\n",
    "    \n",
    "    return queue_w, vector_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(queue_x, queue_y, queue_w, vector_b):\n",
    "    pred_queue_y = np.zeros(queue_y.shape)\n",
    "    phi = softmax(np.dot(queue_x, queue_w) + vector_b)\n",
    "    class_labels = np.argmax(phi, axis = 1)\n",
    "    correct_count = 0\n",
    "    \n",
    "    for i in range(len(queue_y)):\n",
    "        pred_queue_y[i, class_labels[i]] = 1\n",
    "    for i in range(len(queue_y)):\n",
    "        if np.all(queue_y[i, :] == pred_queue_y[i, :]):\n",
    "            correct_count += 1\n",
    "            \n",
    "    return correct_count / len(queue_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(lr_exponent, rp_exponent):\n",
    "    import numpy as np\n",
    "    \n",
    "    features_dic = make_features_dic('train.feature.txt')\n",
    "    train_queue_x, train_queue_y = make_queue('train.txt', features_dic)\n",
    "    valid_queue_x, valid_queue_y = make_queue('valid.txt', features_dic)\n",
    "    lr = pow(10, lr_exponent) # 学習率\n",
    "    rp = pow(10, rp_exponent) # 正則化パラメータ\n",
    "    queue_w = np.ones([len(features_dic), 4])\n",
    "    vector_b = np.ones([1, 4])\n",
    "    accuracy, best_accuracy, pre_best_accuracy = 0, 0, 0\n",
    "    count = 1\n",
    "    \n",
    "    while True:\n",
    "        pre_queue_w = queue_w\n",
    "        pre_vector_b = vector_b\n",
    "        \n",
    "        queue_w, vector_b = train(train_queue_x, train_queue_y, pre_queue_w, pre_vector_b, lr, rp)\n",
    "        accuracy = valid(valid_queue_x, valid_queue_y, queue_w, vector_b)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_queue_w = queue_w\n",
    "            best_vector_b = vector_b\n",
    "            best_accuracy = accuracy\n",
    "            best_count = count\n",
    "        if count % 1000 == 0:\n",
    "            if best_accuracy == pre_best_accuracy:\n",
    "                break\n",
    "            pre_best_accuracy = best_accuracy\n",
    "        count += 1\n",
    "    print('[{0}] {1}\\n'.format(best_count, best_accuracy))\n",
    "            \n",
    "    return best_queue_w, best_vector_b, best_accuracy, best_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(fname, queue_w, vector_b):\n",
    "    features_dic = make_features_dic('train.feature.txt')\n",
    "    queue_x, queue_y = make_queue(fname, features_dic)\n",
    "    accuracy = valid(queue_x, queue_y, queue_w, vector_b)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習率: 10^-5\t正則化パラメータ: 10^0\n",
      "[13670] 0.8950524737631185\n",
      "\n",
      "学習率: 10^-4\t正則化パラメータ: 10^0\n",
      "[3808] 0.9077961019490255\n",
      "\n",
      "学習率: 10^-3\t正則化パラメータ: 10^0\n",
      "[1166] 0.9167916041979011\n",
      "\n",
      "学習率: 10^-2\t正則化パラメータ: 10^0\n",
      "[1610] 0.9070464767616192\n",
      "\n",
      "RESULT\n",
      "学習率: 10^-3\t正則化パラメータ: 10^0\n",
      "評価データの正解率: 0.9092953523238381\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fname = 'test.txt'\n",
    "result_accuracy = 0\n",
    "\n",
    "for i in range(-5, -1):\n",
    "    for j in range(0, 5):\n",
    "        print('学習率: 10^{0}\\t正則化パラメータ: 10^{1}'.format(i, j))\n",
    "        queue_w, vector_b, accuracy, count = make_model(i, j)\n",
    "        if accuracy > result_accuracy:\n",
    "            result_queue_w = queue_w\n",
    "            result_vector_b = vector_b\n",
    "            result_accuracy = accuracy\n",
    "            result_parameter = [i, j]\n",
    "        elif accuracy == result_accuracy and count < result_count:\n",
    "            result_queue_w = queue_w\n",
    "            result_vector_b = vector_b\n",
    "            result_accuracy = accuracy\n",
    "            result_parameter = [i, j]\n",
    "        break # 検証の結果、正則化パラメータは結果に影響しなかったため省略\n",
    "result_accuracy = get_accuracy(fname, result_queue_w, result_vector_b)\n",
    "\n",
    "print('RESULT')\n",
    "print('学習率: 10^{0}\\t正則化パラメータ: 10^{1}'.format(result_parameter[0], result_parameter[1]))\n",
    "print('評価データの正解率: {0}'.format(result_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
