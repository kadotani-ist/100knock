{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_dic(fname):\n",
    "    with open(fname) as file:\n",
    "        return {line.rstrip('\\n'): i for i, line in enumerate(file)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_queue(fname, features_dic):\n",
    "    with open(fname) as file:\n",
    "        lines = file.readlines()\n",
    "        queue_x = np.zeros([len(lines), len(features_dic)])\n",
    "        queue_y = np.zeros([len(lines), 4]) # One-hot表現: [b, t, e, m]\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            import re\n",
    "            from nltk.corpus import stopwords\n",
    "            import snowballstemmer\n",
    "            \n",
    "            line = line.rstrip('\\n')\n",
    "            category = line.strip('\\t')[0]\n",
    "            title = line.split('\\t')[1]\n",
    "            \n",
    "            # queue_xの処理\n",
    "            # 文字種の統一\n",
    "            title = title.lower()\n",
    "            \n",
    "            # 数字の置き換え -> 除去\n",
    "            title = re.sub(r'[0-9]+', '', title)\n",
    "            \n",
    "            # '-'を' 'に変換\n",
    "            title = title.replace('-', ' ')\n",
    "            \n",
    "            words = title.split()\n",
    "            \n",
    "            # ストップワードの除去\n",
    "            stop_words = stopwords.words('english')\n",
    "            words2 = [word for word in words if word not in stop_words]\n",
    "            words = words2\n",
    "            \n",
    "            # ステミング処理\n",
    "            stemmer = snowballstemmer.stemmer('english')\n",
    "            words2 = [stemmer.stemWord(word) for word in words]\n",
    "            words = words2\n",
    "            \n",
    "            # 記号の除去\n",
    "            words2 = [word for word in words if word.islower()]\n",
    "            words = words2\n",
    "            \n",
    "            for word in words:\n",
    "                if word in features_dic.keys():\n",
    "                    queue_x[i, features_dic[word]] = 1\n",
    "                    \n",
    "            # queue_yの処理\n",
    "            if category == 'b':\n",
    "                queue_y[i, 0] = 1\n",
    "            elif category == 't':\n",
    "                queue_y[i, 1] = 1\n",
    "            elif category == 'e':\n",
    "                queue_y[i, 2] = 1\n",
    "            elif category == 'm':\n",
    "                queue_y[i, 3] = 1\n",
    "                \n",
    "    return queue_x, queue_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(queue): # ソフトマックス関数\n",
    "    return np.exp(queue) / np.sum(np.exp(queue), axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(queue_x, queue_y, queue_w, vector_b):\n",
    "    pred_queue_y = np.zeros(queue_y.shape)\n",
    "    phi = softmax(np.dot(queue_x, queue_w) + vector_b)\n",
    "    pred_class_labels = np.argmax(phi, axis = 1)\n",
    "    class_labels = np.argmax(queue_y, axis = 1)\n",
    "    confusion_matrix = np.zeros([4, 4])\n",
    "    \n",
    "    for i in range(len(queue_y)):\n",
    "        confusion_matrix[class_labels[i]][pred_class_labels[i]] += 1\n",
    "            \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(fname):\n",
    "    features_dic = make_features_dic('train.feature.txt')\n",
    "    queue_x, queue_y = make_queue(fname, features_dic)\n",
    "    w_and_b = np.load('results/52_result.npy')\n",
    "    queue_w = w_and_b[:-1]\n",
    "    vector_b = w_and_b[-1]\n",
    "    confusion_matrix = make_confusion_matrix(queue_x, queue_y, queue_w, vector_b)\n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_confusion_matrix = get_confusion_matrix('train.txt')\n",
    "test_confusion_matrix = get_confusion_matrix('test.txt')\n",
    "\n",
    "np.save('results/55_result_train', train_confusion_matrix)\n",
    "np.save('results/55_result_test', test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
